{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T19:30:18.144143Z",
     "start_time": "2025-05-30T19:30:14.628998Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b68ccc89b4c2a59b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T19:38:51.452724Z",
     "start_time": "2025-05-30T19:38:51.447798Z"
    }
   },
   "cell_type": "code",
   "source": "max_len = 50",
   "id": "48b47361d7790e7f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T19:39:15.968452Z",
     "start_time": "2025-05-30T19:39:15.962540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_gpu = True\n",
    "DEVICE = \"cuda\" if use_gpu else \"cpu\"\n",
    "DEVICE"
   ],
   "id": "b3389499bf1fe2e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T19:39:23.633888Z",
     "start_time": "2025-05-30T19:39:23.628487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q1 = \"How can I lose weight?\"\n",
    "q2 = \"What is the best way to reduce body fat?\""
   ],
   "id": "940ea87065496999",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T19:39:24.395578Z",
     "start_time": "2025-05-30T19:39:24.178110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../registry/vocabularies/vocab.json\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "embedding_matrix = torch.load(\"../registry/embeddings/embedding_matrix.pt\")\n",
    "\n",
    "model = QuoraDuplicateGRU(embedding_matrix, hidden_size=100)\n",
    "model.load_state_dict(torch.load(\"../registry/models/gru/gru_model.pt\"))\n",
    "model.eval()\n"
   ],
   "id": "6158ffe203c4bdc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuoraDuplicateGRU(\n",
       "  (embedding): Embedding(50000, 300, padding_idx=0)\n",
       "  (q_encoder): GRU(300, 100, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=800, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T19:39:25.085214Z",
     "start_time": "2025-05-30T19:39:25.076333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_to_sequence(text, vocab, max_len=40):\n",
    "    tokens = text.lower().split()\n",
    "    seq = [vocab.get(token, vocab.get(\"<UNK>\", 0)) for token in tokens]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [vocab[\"<PAD>\"]] * (max_len - len(seq))\n",
    "    return seq[:max_len]\n",
    "\n",
    "q1_tensor = torch.tensor([text_to_sequence(q1, vocab, max_len)[0]], dtype=torch.long).to(DEVICE)\n",
    "q2_tensor = torch.tensor([text_to_sequence(q2, vocab, max_len)[0]], dtype=torch.long).to(DEVICE)"
   ],
   "id": "b4817e6a6dcb18ae",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T19:39:26.920531Z",
     "start_time": "2025-05-30T19:39:26.598513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs=(q1_tensor, q2_tensor),\n",
    "    target=0,\n",
    "    return_convergence_delta=True\n",
    ")"
   ],
   "id": "9fec560fd142bfc9",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcaptum\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mattr\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m IntegratedGradients\n\u001B[32m      3\u001B[39m ig = IntegratedGradients(model)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m attributions, delta = \u001B[43mig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mattribute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq1_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq2_tensor\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_convergence_delta\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m      8\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\captum\\log\\dummy_log.py:39\u001B[39m, in \u001B[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001B[39;00m\n\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001B[39;00m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args: Any, **kwargs: Any):\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:289\u001B[39m, in \u001B[36mIntegratedGradients.attribute\u001B[39m\u001B[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001B[39m\n\u001B[32m    277\u001B[39m     attributions = _batch_attribution(\n\u001B[32m    278\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    279\u001B[39m         num_examples,\n\u001B[32m   (...)\u001B[39m\u001B[32m    286\u001B[39m         method=method,\n\u001B[32m    287\u001B[39m     )\n\u001B[32m    288\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m289\u001B[39m     attributions = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_attribute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    290\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mformatted_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    291\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbaselines\u001B[49m\u001B[43m=\u001B[49m\u001B[43mformatted_baselines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    292\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    293\u001B[39m \u001B[43m        \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m=\u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    294\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_convergence_delta:\n\u001B[32m    299\u001B[39m     start_point, end_point = baselines, inputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:368\u001B[39m, in \u001B[36mIntegratedGradients._attribute\u001B[39m\u001B[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001B[39m\n\u001B[32m    365\u001B[39m expanded_target = _expand_target(target, n_steps)\n\u001B[32m    367\u001B[39m \u001B[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m368\u001B[39m grads = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgradient_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    369\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforward_fn\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscaled_features_tpl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget_ind\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexpanded_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m    \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_additional_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    373\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    375\u001B[39m \u001B[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001B[39;00m\n\u001B[32m    376\u001B[39m \u001B[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001B[39;00m\n\u001B[32m    377\u001B[39m scaled_grads = [\n\u001B[32m    378\u001B[39m     grad.contiguous().view(n_steps, -\u001B[32m1\u001B[39m)\n\u001B[32m    379\u001B[39m     * torch.tensor(step_sizes).float().view(n_steps, \u001B[32m1\u001B[39m).to(grad.device)\n\u001B[32m    380\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m grad \u001B[38;5;129;01min\u001B[39;00m grads\n\u001B[32m    381\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\captum\\_utils\\gradient.py:128\u001B[39m, in \u001B[36mcompute_gradients\u001B[39m\u001B[34m(forward_fn, inputs, target_ind, additional_forward_args)\u001B[39m\n\u001B[32m    110\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[33;03mComputes gradients of the output with respect to inputs for an\u001B[39;00m\n\u001B[32m    112\u001B[39m \u001B[33;03marbitrary forward function.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    124\u001B[39m \u001B[33;03m                arguments) if no additional arguments are required\u001B[39;00m\n\u001B[32m    125\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.autograd.set_grad_enabled(\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m    127\u001B[39m     \u001B[38;5;66;03m# runs forward pass\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     outputs = \u001B[43m_run_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mforward_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_ind\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    129\u001B[39m     \u001B[38;5;66;03m# _run_forward may return future of Tensor,\u001B[39;00m\n\u001B[32m    130\u001B[39m     \u001B[38;5;66;03m# but we don't support it here now\u001B[39;00m\n\u001B[32m    131\u001B[39m     \u001B[38;5;66;03m# And it will fail before here.\u001B[39;00m\n\u001B[32m    132\u001B[39m     outputs = cast(Tensor, outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\captum\\_utils\\common.py:588\u001B[39m, in \u001B[36m_run_forward\u001B[39m\u001B[34m(forward_func, inputs, target, additional_forward_args)\u001B[39m\n\u001B[32m    585\u001B[39m inputs = _format_inputs(inputs)\n\u001B[32m    586\u001B[39m additional_forward_args = _format_additional_forward_args(additional_forward_args)\n\u001B[32m--> \u001B[39m\u001B[32m588\u001B[39m output = \u001B[43mforward_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    589\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    590\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# pyre-fixme[60]: Concatenation not yet support for multiple variadic\u001B[39;49;00m\n\u001B[32m    591\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#  tuples: `*inputs, *additional_forward_args`.\u001B[39;49;00m\n\u001B[32m    592\u001B[39m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    593\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m    594\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    595\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    596\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    597\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, torch.futures.Future):\n\u001B[32m    598\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output.then(\u001B[38;5;28;01mlambda\u001B[39;00m x: _select_targets(x.value(), target))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\duplicate_question_classifier_assigment\\modeling\\networks\\quora_duplicate_gru.py:69\u001B[39m, in \u001B[36mQuoraDuplicateGRU.forward\u001B[39m\u001B[34m(self, q1_input, q2_input)\u001B[39m\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, q1_input, q2_input):\n\u001B[32m     59\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     60\u001B[39m \u001B[33;03m    Forward pass for a pair of questions.\u001B[39;00m\n\u001B[32m     61\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     67\u001B[39m \u001B[33;03m        torch.Tensor: Logits for binary classification, shape (batch_size).\u001B[39;00m\n\u001B[32m     68\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m     q1_repr = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencode_question\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq1_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m     q2_repr = \u001B[38;5;28mself\u001B[39m.encode_question(q2_input)\n\u001B[32m     72\u001B[39m     combined = torch.cat(\n\u001B[32m     73\u001B[39m         [q1_repr, q2_repr, torch.abs(q1_repr - q2_repr), q1_repr * q2_repr],\n\u001B[32m     74\u001B[39m         dim=\u001B[32m1\u001B[39m\n\u001B[32m     75\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\duplicate_question_classifier_assigment\\modeling\\networks\\quora_duplicate_gru.py:53\u001B[39m, in \u001B[36mQuoraDuplicateGRU.encode_question\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mencode_question\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     44\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[33;03m    Encodes a question using BiGRU.\u001B[39;00m\n\u001B[32m     46\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     51\u001B[39m \u001B[33;03m        torch.Tensor: Encoded question representation of shape (batch_size, hidden_size * 2).\u001B[39;00m\n\u001B[32m     52\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m     embedded = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     54\u001B[39m     _, hidden = \u001B[38;5;28mself\u001B[39m.q_encoder(embedded)\n\u001B[32m     55\u001B[39m     hidden = torch.cat((hidden[\u001B[32m0\u001B[39m], hidden[\u001B[32m1\u001B[39m]), dim=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001B[39m, in \u001B[36mEmbedding.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    191\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    192\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\quora-private\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001B[39m, in \u001B[36membedding\u001B[39m\u001B[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[39m\n\u001B[32m   2545\u001B[39m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[32m   2546\u001B[39m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[32m   2547\u001B[39m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[32m   2548\u001B[39m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[32m   2549\u001B[39m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[32m   2550\u001B[39m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[32m-> \u001B[39m\u001B[32m2551\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "10540412defecd8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
